from typing import Any, Iterator

import torch
import torch.func
from torch.utils.data import DataLoader

from src.client.fedavg import FedAvgClient
from src.utils.dp_mechanisms import (
    add_gaussian_noise,
    clip_gradients
)


class DPFedAvgLocalClient(FedAvgClient):
    """Local Differential Privacy FedAvg Client with Per-Sample Gradients.
    
    This client implements local differential privacy using per-sample gradient
    computation. Each sample's gradients are computed and clipped independently,
    then averaged and noised before parameter updates.
    """
    
    def __init__(self, **commons):
        super().__init__(**commons)
        self.iter_trainloader = None
        
        # Initialize DP parameters
        self.clip_norm = self.args.dp_fedavg_local.clip_norm
        self.sigma = self.args.dp_fedavg_local.sigma
        
        # Initialize parameter caching for optimization
        self._param_name_to_param = {}
        self._trainable_params = []
    
    def set_parameters(self, package: dict[str, Any]):
        super().set_parameters(package)
        self.iter_trainloader = iter(self.trainloader)
        # Cache model parameters for optimized access
        self._cache_model_parameters()
    
    def fit(self):
        """Train the model with local differential privacy using per-sample gradients.
        
        This method implements the per-sample DP-SGD algorithm:
        1. Compute per-sample gradients using manual loop method
        2. Clip each sample's gradients independently
        3. Average the clipped gradients
        4. Add calibrated Gaussian noise
        5. Apply noisy gradients
        """
        self.model.train()
        self.dataset.train()
        
        for _ in range(self.local_epoch):
            x, y = self.get_data_batch()
            
            # Compute per-sample gradients
            per_sample_grads = self._compute_per_sample_gradients(x, y)
            
            # Clip per-sample gradients
            clipped_grads = self._clip_per_sample_gradients(per_sample_grads)
            
            # Clear existing gradients and apply averaged gradients with noise
            self.optimizer.zero_grad()
            self._apply_gradients_with_noise(clipped_grads)
            
            # Optimizer step
            self.optimizer.step()
            
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()
    
    def get_data_batch(self):
        # Initialize iterator if not already done
        if self.iter_trainloader is None:
            self.iter_trainloader = iter(self.trainloader)
            
        try:
            x, y = next(self.iter_trainloader)
            if len(x) <= 1:
                x, y = next(self.iter_trainloader)
        except StopIteration:
            self.iter_trainloader = iter(self.trainloader)
            x, y = next(self.iter_trainloader)
        return x.to(self.device), y.to(self.device)
    
    def _cache_model_parameters(self):
        """Cache model parameters to avoid repeated traversals.
        
        Builds mappings from parameter names to parameter objects and
        creates a list of trainable parameters for efficient access.
        """
        self._param_name_to_param.clear()
        self._trainable_params.clear()
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self._param_name_to_param[name] = param
                self._trainable_params.append(param)
    
    
    
    def _compute_grad_norm(self, grad_dict):
        """Compute the L2 norm of gradients in a dictionary.
        
        Args:
            grad_dict: Dictionary of parameter names to gradients
            
        Returns:
            float: L2 norm of all gradients
        """
        total_norm = 0.0
        for grad in grad_dict.values():
            if grad is not None:
                param_norm = grad.detach().norm()
                total_norm += param_norm.item() ** 2
        return total_norm ** 0.5
    
    def _compute_per_sample_gradients(self, x, y):
        """Compute per-sample gradients using manual loop method.
        
        This method computes gradients for each sample independently by iterating
        through the batch and performing individual forward/backward passes.
        This approach is compatible with models containing in-place operations
        that are not supported by torch.func.
        
        Args:
            x: Input batch tensor
            y: Target batch tensor
            
        Returns:
            list: List of gradient dictionaries, one per sample
        """
        batch_size = x.shape[0]
        grad_list = []
        
        # Store original gradients state
        original_grads = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                original_grads[name] = param.grad.clone() if param.grad is not None else None
        
        try:
            # Process each sample individually
            for i in range(batch_size):
                # Clear gradients
                self.optimizer.zero_grad()
                
                # Single sample forward pass
                x_single = x[i:i+1]  # Keep batch dimension
                y_single = y[i:i+1]  # Keep batch dimension
                
                # Forward pass
                logits = self.model(x_single)
                loss = self.criterion(logits, y_single)
                
                # Backward pass
                retain_graph = (i < batch_size - 1)  # Retain graph for all but last sample
                loss.backward(retain_graph=retain_graph)
                
                # Extract and store gradients for this sample
                sample_grad = self._extract_sample_gradients()
                grad_list.append(sample_grad)
            
            return grad_list
            
        finally:
            # Restore original gradients
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    param.grad = original_grads[name]
    
    def _extract_sample_gradients(self):
        """Extract current gradients from model parameters.
        
        Returns:
            dict: Dictionary mapping parameter names to gradient tensors
        """
        sample_grad = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                sample_grad[name] = param.grad.clone()
        return sample_grad
    
    def _clip_per_sample_gradients(self, per_sample_grads):
        """Clip each sample's gradients independently.
        
        Args:
            per_sample_grads: List of gradient dictionaries, one per sample
            
        Returns:
            list: List of clipped gradient dictionaries
        """
        clipped_grads = []
        for sample_grad in per_sample_grads:
            # Compute gradient norm for this sample
            grad_norm = self._compute_grad_norm(sample_grad)
            
            # Clip if necessary
            if grad_norm > self.clip_norm:
                scaling_factor = self.clip_norm / grad_norm
                clipped_sample_grad = {}
                for name, grad in sample_grad.items():
                    clipped_sample_grad[name] = grad * scaling_factor
                clipped_grads.append(clipped_sample_grad)
            else:
                clipped_grads.append(sample_grad)
                
        return clipped_grads
    
    def _apply_averaged_gradients(self, clipped_grads):
        """Average the per-sample gradients and apply to model parameters.
        
        Args:
            clipped_grads: List of clipped gradient dictionaries
        """
        if not clipped_grads:
            return
            
        # Stack and average gradients for each parameter using cached mappings
        for name in clipped_grads[0].keys():
            if name in self._param_name_to_param:
                # Stack all per-sample gradients and take mean along batch dimension
                grad_stack = torch.stack([sample_grad[name] for sample_grad in clipped_grads])
                averaged_grad = grad_stack.mean(dim=0)
                # Directly apply using cached parameter reference
                self._param_name_to_param[name].grad = averaged_grad
    
    def _apply_gradients_with_noise(self, clipped_grads):
        """Apply averaged gradients and add noise in a single traversal.
        
        This method combines gradient application and noise addition to optimize
        performance by reducing parameter traversals from 2 to 1.
        
        Args:
            clipped_grads: List of clipped gradient dictionaries
        """
        if not clipped_grads:
            return
        
        # Initialize parameter cache if not already done
        if not self._param_name_to_param:
            self._cache_model_parameters()
            
        # Single traversal: average gradients and add noise
        for name in clipped_grads[0].keys():
            if name in self._param_name_to_param:
                param = self._param_name_to_param[name]
                
                # Stack all per-sample gradients and take mean along batch dimension
                grad_stack = torch.stack([sample_grad[name] for sample_grad in clipped_grads])
                averaged_grad = grad_stack.mean(dim=0)
                
                # Add Gaussian noise to the averaged gradient
                noisy_grad = add_gaussian_noise(
                    averaged_grad,
                    sigma=self.sigma,
                    device=param.device
                )
                
                # Apply the noisy gradient to the parameter
                param.grad = noisy_grad
    
    

    
    
    def package(self):
        """Package client data including DP parameters."""
        client_package = super().package()
        
        # Add DP parameters for server tracking
        client_package["dp_parameters"] = {
            "clip_norm": self.clip_norm,
            "sigma": self.sigma
        }
        
        return client_package
